{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context\n",
    "Satellite imagery provides unique insights into various markets, including agriculture, defense and intelligence, energy, and finance. New commercial imagery providers, such as Planet and BlackSky, are using constellations of small satellites to exponentially increase the amount of images of the earth captured every day.\n",
    "\n",
    "This flood of new imagery is outgrowing the ability for organizations to manually look at each image that gets captured, and there is a need for machine learning and computer vision algorithms to help automate the analysis process.\n",
    "\n",
    "The aim of this dataset is to help address the common task of detecting the presense of large ships in satellite images. Automating this process can be applied to many issues including monitoring port activity levels and supply chain analysis.\n",
    "\n",
    "The dataset consists of image chips extracted from Planet satellite imagery collected over the San Franciso Bay area. It includes 2800 80x80 RGB images labeled with either a \"ship\" or \"no-ship\" classification. Image chips were derived from PlanetScope full-frame visual scene products, which are orthorectified to a 3 meter pixel size.\n",
    "\n",
    "Continusouly updates will be made to this dataset as new Planet imagery released. Current images were collected as late as September 2017.\n",
    "\n",
    "Content\n",
    "Provided is a zipped directory shipsnet.7z that contains the entire dataset as .png image chips. Each individual image filename follows a specific format: {label} __ {scene id} __ {longitude} _ {latitude}.png\n",
    "\n",
    "label: Valued 1 or 0, representing the \"ship\" class and \"no-ship\" class, respectively.\n",
    "scene id: The unique identifier of the PlanetScope visual scene the image chip was extracted from. The scene id can be used with the Planet API to discover and download the entire scene.\n",
    "longitude_latitude: The longitude and latitude coordinates of the image center point, with values separated by a single underscore.\n",
    "The dataset is also distributed as a JSON formatted text file shipsnet.json. The loaded object contains data, label, scene_ids, and location lists.\n",
    "\n",
    "The pixel value data for each 80x80 RGB image is stored as a list of 19200 integers within of the data list. The first 6400 entries contain the red channel values, the next 6400 the green, and the final 6400 the blue. The image is stored in row-major order, so that the first 80 entries of the array are the red channel values of the first row of the image.\n",
    "\n",
    "The list values at index i in labels, scene_ids, and locations each correspond to the i-th image in the data list.\n",
    "\n",
    "Class Labels\n",
    "The \"ship\" class includes 700 images. Images in this class are near-centered on the body of a single ship. Ships of different ship sizes, orientations, and atmospheric collection conditions are included. Example images from this class are shown below.\n",
    "<img src='../course_material/images/ship.png'>\n",
    "\n",
    "The \"no-ship\" class includes 2100 images. A third of these are a random sampling of different landcover features - water, vegetion, bare earth, buildings, etc. - that do not include any portion of an ship. The next third are \"partial ships\" that contain only a portion of an ship, but not enough to meet the full definition of the \"ship\" class. The last third are images that have previously been mislabeled by machine learning models, typically caused by bright pixels or string linear features. Example images from this class are shown below.\n",
    "<img src='../course_material/images/nonship.png'>\n",
    "Q3daQMC.png\n",
    "Acknowledgements\n",
    "Satellite imagery used to build PlanesNet is made available through Planet's Open California dataset, which is openly licensed. As such, this dataset is also available under the same CC-BY-SA license. Users can sign up for a free Planet account to search, view, and download thier imagery and gain access to their API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils.np_utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(r'./data/shipsnet.json')\n",
    "dataset = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.array(dataset['data']).astype('uint8')\n",
    "labels = np.array(dataset['labels']).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data / 255.\n",
    "x = x.reshape([-1, 3, 80, 80]).transpose([0,2,3,1])\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels, num_classes=2)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id_to_check = np.random.randint(0, x.shape[0] - 1)\n",
    "im = x[img_id_to_check]\n",
    "\n",
    "print(img_id_to_check)\n",
    "print(y[img_id_to_check])\n",
    "\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
