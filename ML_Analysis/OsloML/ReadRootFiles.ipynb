{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "#from ROOT import TH1F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follwoing file (*infofile.py*) loads a python dictionary (*info*) which stores all the cross-sections and number of events needed to scale each MC process to the right luminosity. The scale factor is given by\n",
    "\n",
    "$$sf_{\\mathscr{L}} = \\frac{\\sigma[fb]*\\mathscr{L}}{N_{ev}^{simulated}}.$$\n",
    "\n",
    "$N_{ev}^{simulated}$ is the number of originally simulated events for a given MC sample, $\\sigma$ is the cross-section in femtobarns [fb] and $\\mathscr{L}$ is the integrated luminosity of the data sample. For the 13TeV openData release $\\mathscr{L} = 10.6~fb^{-1}$. Note that the cross-section in the infofile is in picobarn [pb] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from infofile import infos\n",
    "info = {} \n",
    "for key in infos.keys(): \n",
    "    ID = infos[key]['DSID']\n",
    "    info[ID] = {} \n",
    "    info[ID]['xsec'] = infos[key]['xsec'] \n",
    "    info[ID]['sumw'] = infos[key]['sumw'] \n",
    "    info[ID]['events'] = infos[key]['events']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the luminosity of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lumi = 10.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lists below define the DSIDs to be identified with each background type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUSYC1C1 = [392501,392502,392504,392506,392507,392509,392513,392517,392518,392521]\n",
    "\n",
    "SUSYslsl = [392916,392918,392920,392924,392925,392936,392942,392951,392962,392964,392982,392985,392996,392999]\n",
    "\n",
    "Zjets = [364100, 364101, 364102, 364103, 364104, 364105, 364106, 364107, 364108, 364109, 364110, \n",
    "         364111, 364112, 364113, 364114, 364115, 364116, 364117, 364118, 364119, 364120, 364121, \n",
    "         364122, 364123, 364124, 364125, 364126, 364127, 364128, 364129, 364130, 364131, 364132, \n",
    "         364133, 364134, 364135, 364136, 364137, 364138, 364139, 364140, 364141]\n",
    "\n",
    "Wjets = [364156, 364157, 364158, 364159, 364160, 364161, 364162, 364163, 364164, 364165, 364166, \n",
    "         364167, 364168, 364169, 364170, 364171, 364172, 364173, 364174, 364175, 364176, 364177, \n",
    "         364178, 364179, 364180, 364181, 364182, 364183, 364184, 364185, 364186, 364187, 364188, \n",
    "         364189, 364190, 364191, 364192, 364193, 364194, 364195, 364196, 364197]\n",
    "\n",
    "Diboson = [361600, 361601, 361602, 361603, 361604, 361606, 361607, 361609, 361610] \n",
    "\n",
    "Top = [410000, 410011, 410012, 4100013, 410014, 410025, 410026]\n",
    "\n",
    "Higgs = [341081, 343981, 345041, 345318, 345319]\n",
    "\n",
    "fileIDs_bkg = {'Diboson':Diboson, 'Zjets':Zjets, 'Wjets':Wjets, 'Top':Top, 'Higgs':Higgs}\n",
    "fileIDs_sig = {'SUSYC1C1':SUSYC1C1, 'SUSYslsl':SUSYslsl}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the scale factor needed for the scaling of MC to the right luminosity. In stead of storing all the individual scale factors, weights and cross-sections this function calculates the final scale factor so that the individual branches (i.e. scaleFactor_\\*, mc_weight, etc.) do not need to be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sf(xsec,lumi,nev,mcWeight,scaleFactor_PILEUP,scaleFactor_ELE,scaleFactor_MUON,scaleFactor_BTAG,scaleFactor_LepTRIGGER):\n",
    "    if lumi <= 0: \n",
    "        print(\"Lumi {:d} is not valid\".format(lumi)) \n",
    "        return 0\n",
    "    wgt = (mcWeight)*(scaleFactor_PILEUP)*(scaleFactor_ELE)*(scaleFactor_MUON)*(scaleFactor_BTAG)*(scaleFactor_LepTRIGGER)\n",
    "    return wgt * ((xsec*lumi)/nev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function specify the skimming. Due to memory consumption it is not feasible to read all events so some skimming is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skimming(lep_n,lep_pt,met):\n",
    "    if met < 50000: return True\n",
    "    if lep_n != 2: return True\n",
    "    if lep_pt[0] < 25000: return True\n",
    "    if lep_pt[1] < 25000: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the branches to keep in the hd5 files. If you like to check all the availale brances open a file and do events.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branches = ['eventNumber','channelNumber',\n",
    "            'mcWeight','scaleFactor_PILEUP','scaleFactor_ELE','scaleFactor_MUON',\n",
    "            'scaleFactor_BTAG','scaleFactor_LepTRIGGER',\n",
    "            'lep_*','met_*','jet_n']#'jet_*']#,'photon_*','fatjet_*','tau_*',\n",
    "            #'ditau_m','truth_*']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters should be set to specify how many events will be read in every chunk, the directory for the files you would like to convert into hd5 files and a tag wich will be appended to the output filename to reflect the skimming applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many events to read in each round\n",
    "chunksize = 500000\n",
    "# How often to write to hd5 file (the smaller, the more files)\n",
    "printevry = 500000\n",
    "# Directory containing the input file (must end with .root)\n",
    "#indir   = \"/scratch3/eirikgr/openData_13TeV/Data\"\n",
    "indir   = \"/scratch3/eirikgr/openData_13TeV/MC/SM_Backgrounds/\"\n",
    "# The output file name tag to store the skim options used abvove\n",
    "datatype = indir.split(\"/\")[-1]\n",
    "# In cases there is a trailing / at the end\n",
    "if not datatype: datatype = indir.split(\"/\")[-2]\n",
    "print(\"INFO \\t Data type is {:s}\".format(datatype))\n",
    "\n",
    "# Define if it is signal or background (important for ML classification)\n",
    "isMC = False\n",
    "isData = False\n",
    "isSignal = False\n",
    "if datatype == \"SM_Backgrounds\":\n",
    "    isMC = True\n",
    "elif datatype == \"BSM_Signal_Samples\":\n",
    "    isSignal = True\n",
    "elif datatype == \"Data\":\n",
    "    isData = True\n",
    "else:\n",
    "    print(\"ERROR \\t Datatype {:s} is unknown. Setting as background\".format(datatype))\n",
    "    isMC = True\n",
    "    \n",
    "skimtag = \"2L_pt25_25_met50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count how many events and files we write/read\n",
    "n_allfiles   = 0\n",
    "nfile   = 0\n",
    "totev   = 0\n",
    "totev_skim = 0\n",
    "out_filenum = 1\n",
    "try:\n",
    "    del [result]\n",
    "except:\n",
    "    print(\"WARNING\\t Result does not exists. Good :-)\")\n",
    "root_files = [f for f in listdir(indir) if f.endswith('.root')]\n",
    "\n",
    "# Checking if file exitst (not needed)\n",
    "#path = indir+\"/%s_%s.h5\" %(datatype,skimtag)\n",
    "#if os.path.exists(path):\n",
    "#    os.remove(path)\n",
    "#    print(\"WARNING\\t Removing file {:s}\".format(path))\n",
    "\n",
    "for f in root_files:\n",
    "    n_allfiles += 1\n",
    "    # Getting the file and extracting the information from info dictionary\n",
    "    events = uproot.open(indir+\"/\"+f)[\"mini\"]\n",
    "    nentries = events.numentries\n",
    "    print(\"INFO  \\t Opening file {:d}/{:d}: {:s} with {:d} events\".format(n_allfiles,len(root_files),f,nentries))\n",
    "    path = indir+\"/%s_%s.h5\" %(f,skimtag)\n",
    "    if not isData:\n",
    "        file_id = int(f.split('.')[1])\n",
    "        if not file_id in info.keys():\n",
    "            print(\"ERROR \\t Could not find any info for file id {:d}. Skipping.\".format(file_id))\n",
    "            continue\n",
    "        xsec = float(info[file_id]['xsec'])\n",
    "        nev  = float(info[file_id]['sumw'])\n",
    "        \n",
    "    \n",
    "        # Find the MC type (defined by dictonaries a few cells above)\n",
    "        mccat = \"\" \n",
    "        if isMC:\n",
    "            for key in fileIDs_bkg.keys():\n",
    "                if file_id in fileIDs_bkg[key]:       \n",
    "                    mccat = key\n",
    "                    break\n",
    "        elif isSignal:\n",
    "            for key in fileIDs_sig.keys():\n",
    "                if file_id in fileIDs_sig[key]:       \n",
    "                    mccat = key\n",
    "                    break\n",
    "        if not mccat:\n",
    "            print(\"ERROR \\t Could not find category for DSID {:d}. Skipping\".format(file_id))\n",
    "            continue\n",
    "        print(\"INFO  \\t ID {:d} in category {:s} has xsec = {:.1f} fb and nev = {:.2f} \".format(file_id,mccat,xsec,nev))\n",
    "    \n",
    "    \n",
    "    n = 1\n",
    "    prev_n = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # Measure time to read \n",
    "        if n == 1: start = time.time()   \n",
    "        else:   \n",
    "            end = time.time()\n",
    "            dur = (end - start)\n",
    "            dur_sec = chunksize/dur\n",
    "            m, s = divmod((nentries-((n-1)*chunksize))/dur_sec, 60)\n",
    "            h, m = divmod(m, 60)\n",
    "            print(\"INFO  \\t Event/sec  = {:.0f}. ETC = {:d}h{:02d}m{:02d}s\".format(dur_sec,int(h),int(m),int(s)))\n",
    "            start = time.time()\n",
    "        \n",
    "        # Get the range of events to read\n",
    "        next_n = n*chunksize if n*chunksize < nentries else nentries\n",
    "        print(\"INFO  \\t Reading entries {:d} - {:d} of {:d}. Total so far: {:d}\".format(prev_n,next_n,nentries,totev_skim))\n",
    "        df = events.pandas.df(branches,flatten=False,entrystart=prev_n,entrystop=n*chunksize if n*chunksize < nentries else nentries)\n",
    "        \n",
    "        totev += len(df.index)\n",
    "        \n",
    "        # If MC: get the scale factor corresponding to the total data luminosity (is set above). \n",
    "        # If data: set scale factor to 1!\n",
    "        if not \"data\" in f:\n",
    "            df['wgt'] = np.vectorize(calc_sf)(xsec,lumi,nev,df.mcWeight,\n",
    "                                              df.scaleFactor_PILEUP,df.scaleFactor_ELE,\n",
    "                                              df.scaleFactor_MUON,df.scaleFactor_BTAG,\n",
    "                                              df.scaleFactor_LepTRIGGER)\n",
    "        else:\n",
    "            df['wgt'] = 1.0\n",
    "\n",
    "        # Define if it is signal or background (important for ML classification)\n",
    "        if isMC or isData:\n",
    "            df['isSignal'] = False\n",
    "        elif isSignal:\n",
    "            df['isSignal'] = True\n",
    "        else:\n",
    "            df['isSignal'] = False\n",
    "            \n",
    "        df['MCType'] = mccat\n",
    "        \n",
    "        ###########   \n",
    "        # Skimming\n",
    "        ###########  \n",
    "        failskim = df[np.vectorize(skimming)(df.lep_n,df.lep_pt,df.met_et)].index\n",
    "        print(\"INFO  \\t Dropping {:d} events ({:.1f}%)\".format(len(failskim),(len(failskim)/len(df.index))*100.))\n",
    "        df.drop(failskim,inplace=True)\n",
    "        \n",
    "        ###########   \n",
    "        # Slimming\n",
    "        ###########\n",
    "        df = df.drop(['lep_trigMatched','lep_truthMatched','mcWeight','scaleFactor_PILEUP','scaleFactor_ELE','scaleFactor_MUON','scaleFactor_BTAG','scaleFactor_LepTRIGGER'],axis=1)\n",
    "\n",
    "        # If first time, create result data frame. If not; concatenate\n",
    "        try: \n",
    "            result = pd.concat([result,df])\n",
    "        except:\n",
    "            result = df\n",
    "            print(\"WARNING\\t Starting a new result panda\")\n",
    "            \n",
    "        totev_skim += len(df.index)\n",
    "\n",
    "        # Delete the temporary data frame\n",
    "        del [df]\n",
    "        \n",
    "        if totev_skim > printevry:\n",
    "            path = indir+\"/%s_%s_num_%i.h5\" %(datatype,skimtag,out_filenum)\n",
    "            result.to_hdf(path,'mini',mode='w',table=True)\n",
    "            print(\"INFO  \\t Read {:d} events in {:d} files, for which {:d} ({:.2f}%) were written to {:s}\"\n",
    "                .format(totev,nfile,totev_skim,(float(totev_skim)/float(totev))*100.,path))\n",
    "            out_filenum += 1\n",
    "            totev = 0\n",
    "            totev_skim = 0\n",
    "            nfile = 0\n",
    "            del [result]\n",
    "        \n",
    "        # If read everything\n",
    "        if n*chunksize > nentries: break\n",
    "\n",
    "        # Update counters before continuing\n",
    "        prev_n = n*chunksize + 1\n",
    "        n += 1\n",
    "        \n",
    "    nfile += 1\n",
    "    del [events]  \n",
    "    \n",
    "# Make sure we write the last events\n",
    "path = indir+\"/%s_%s_num_%i.h5\" %(datatype,skimtag,out_filenum)\n",
    "result.to_hdf(path,'mini',mode='w',table=True)\n",
    "print(\"INFO  \\t Read {:d} events in {:d} files, for which {:d} ({:.2f}%) were written to {:s}\"\n",
    "    .format(totev,nfile,totev_skim,(float(totev_skim)/float(totev))*100.,path))\n",
    "del [result]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
