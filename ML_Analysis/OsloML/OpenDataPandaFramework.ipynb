{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "import os\n",
    "import h5py\n",
    "import awkward\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follwoing file (*infofile.py*) loads a python dictionary (*info*) which stores all the cross-sections and number of events needed to scale each MC process to the right luminosity. The scale factor is given by\n",
    "\n",
    "$$sf_{\\mathscr{L}} = \\frac{\\sigma[fb]*\\mathscr{L}}{N_{ev}^{simulated}}.$$\n",
    "\n",
    "$N_{ev}^{simulated}$ is the number of originally simulated events for a given MC sample, $\\sigma$ is the cross-section in femtobarns [fb] and $\\mathscr{L}$ is the integrated luminosity of the data sample. For the 13TeV openData release $\\mathscr{L} = 10.6~fb^{-1}$. Note that the cross-section in the infofile is in picobarn [pb] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from infofile import infos\n",
    "info = {} \n",
    "for key in infos.keys(): \n",
    "    ID = infos[key]['DSID']\n",
    "    info[ID] = {} \n",
    "    info[ID]['xsec'] = infos[key]['xsec'] \n",
    "    info[ID]['sumw'] = infos[key]['sumw'] \n",
    "    info[ID]['events'] = infos[key]['events']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lists below define the DSIDs to be identified with each background type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUSYC1C1 = [392501,392502,392504,392506,392507,392509,392513,392517,392518,392521]\n",
    "\n",
    "SUSYslsl = [392916,392918,392920,392924,392925,392936,392942,392951,392962,392964,392982,392985,392996,392999]\n",
    "\n",
    "ZPrime_chi = [301322,301323,301324,301325,301326,301327,301328,301329,301330,301331,301332,301333]\n",
    "\n",
    "Zjets = [364100, 364101, 364102, 364103, 364104, 364105, 364106, 364107, 364108, 364109, 364110, \n",
    "         364111, 364112, 364113, 364114, 364115, 364116, 364117, 364118, 364119, 364120, 364121, \n",
    "         364122, 364123, 364124, 364125, 364126, 364127, 364128, 364129, 364130, 364131, 364132, \n",
    "         364133, 364134, 364135, 364136, 364137, 364138, 364139, 364140, 364141]\n",
    "\n",
    "Wjets = [364156, 364157, 364158, 364159, 364160, 364161, 364162, 364163, 364164, 364165, 364166, \n",
    "         364167, 364168, 364169, 364170, 364171, 364172, 364173, 364174, 364175, 364176, 364177, \n",
    "         364178, 364179, 364180, 364181, 364182, 364183, 364184, 364185, 364186, 364187, 364188, \n",
    "         364189, 364190, 364191, 364192, 364193, 364194, 364195, 364196, 364197]\n",
    "\n",
    "Diboson = [361600, 361601, 361602, 361603, 361604, 361606, 361607, 361609, 361610] \n",
    "\n",
    "Top = [410000, 410011, 410012, 4100013, 410014, 410025, 410026]\n",
    "\n",
    "Higgs = [341081, 343981, 345041, 345318, 345319]\n",
    "\n",
    "fileIDs_bkg = {'Diboson':Diboson, 'Zjets':Zjets, 'Wjets':Wjets, 'Top':Top, 'Higgs':Higgs}\n",
    "fileIDs_sig = {'SUSYC1C1':SUSYC1C1, 'SUSYslsl':SUSYslsl, 'ZPrime_chi':ZPrime_chi}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the scale factor needed for the scaling of MC to the right luminosity. In stead of storing all the individual scale factors, weights and cross-sections this function calculates the final scale factor so that the individual branches (i.e. scaleFactor_*, mc_weight, etc.) do not need to be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sf(xsec,lumi,nev,mcWeight,scaleFactor_PILEUP,scaleFactor_ELE,scaleFactor_MUON,scaleFactor_BTAG,scaleFactor_LepTRIGGER):\n",
    "    if lumi <= 0: \n",
    "        print(\"Lumi {:d} is not valid\".format(lumi)) \n",
    "        return 0\n",
    "    wgt = (mcWeight)*(scaleFactor_PILEUP)*(scaleFactor_ELE)*(scaleFactor_MUON)*(scaleFactor_BTAG)*(scaleFactor_LepTRIGGER)\n",
    "    return wgt * ((xsec*1000*lumi)/nev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follwoing function is not really used anymore, but maybe it will become useful at some point. So saves it here :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortAndIndex(df,nlep):\n",
    "    # Removes the subentry index (will be replaced by a sorted (in terms of lepton pt) index)\n",
    "    df.reset_index(level=1, drop=True, inplace=True)\n",
    "    df = df.set_index(['lep_pt'],append=True).sort_index(ascending=False)\n",
    "    if (df.shape[0] % nlep) != 0:\n",
    "        print(\"WARNING\\t Shape is odd {:d}\".format(df.shape[0]))\n",
    "        size  = int(df.shape[0]/nlep)+1\n",
    "    else:\n",
    "        size  = int(df.shape[0]/nlep)\n",
    "    serie = [x for x in range(1,nlep+1)]\n",
    "    s = pd.Series(serie*size)\n",
    "    if s.shape[0] != df.shape[0]:\n",
    "        print(\"ERROR \\t Shape mismatch serie: {:d} vs. data frame: {:d}\".format(s.shape[0],df.shape[0]))\n",
    "        print(s)\n",
    "    df.set_index([s],append=True,inplace=True)\n",
    "    df.reset_index(level=1,inplace=True)\n",
    "    df.rename_axis(('entry','lepnum'),inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skimming\n",
    "The following function specify the skimming. Due to memory consumption it is not feasible to read all events so some skimming is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skimming(df,events,prev_n,next_n,nlep,lep_ptcut):\n",
    "    #pt, met = events.arrays(['lep_pt','met_et'],outputtype=tuple,entrystart=prev_n,entrystop=next_n)\n",
    "    # Vector to hold all the \n",
    "    \n",
    "    pt  = awkward.fromiter(df['lep_pt'])\n",
    "    met = awkward.fromiter(df['met_et'])\n",
    "    \n",
    "    skim = []\n",
    "    \n",
    "    # MET > 50 GeV\n",
    "    skim.append(pd.Series(met > 50000,name='bools').astype(int))\n",
    "    \n",
    "    # Makes sure that each sub-vector has the same number of entries (filling -999 if not)\n",
    "    pt = pt.pad(nlep).fillna(-999)\n",
    "    \n",
    "    # Require leptons to have enough pt (according to values set in lep_ptcut)\n",
    "    for i in range(len(lep_ptcut)):\n",
    "        se = pt > lep_ptcut[i]\n",
    "        skim.append(pd.Series(pt[pt > lep_ptcut[i]].counts >= 1).astype(int))\n",
    "        mask = np.logical_and(pt != pt.max(), pt.max != -999)\n",
    "        pt = pt[mask]\n",
    "        \n",
    "     \n",
    "    # Make sure we only have exactly nlep (after the pt cuts)\n",
    "    skim.append(pd.Series(pt[pt > 0].counts == 0).astype(int))\n",
    "    \n",
    "        \n",
    "    # < here one can add additional cuts. Remeber to append the result to the skim vector)\n",
    "    \n",
    "    # Make sure that all our entries in the skim vector has value 1 \n",
    "    # If not this means that one of the cuts above did not pass (i.e. we don't want to keep the event)\n",
    "    # Adding values from all skim vectors together should give a total equal to the length of the skim vector\n",
    "    sk_final = skim[0]\n",
    "    for i in range(1,len(skim)):\n",
    "        sk_final = sk_final.add(skim[i])\n",
    "    final_skim = pd.Series(sk_final == len(skim))\n",
    "    \n",
    "    # Keep only rows where we have right number of leptons with pT above thresholds\n",
    "    df = df[final_skim.values]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation\n",
    "The following function let you add new jet variables into the panda data frame. The jet information is removed apriori as the exact number of jets is varying from event to event. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jetaugmentation(df,events,prev_n,next_n):\n",
    "    #pt, eta, phi, e = events.arrays(['jet_pt','jet_eta','jet_phi','jet_E'],outputtype=tuple,entrystart=prev_n,entrystop=next_n)\n",
    "    pt  = awkward.fromiter(df['jet_pt'])\n",
    "    eta = awkward.fromiter(df['jet_eta'])\n",
    "    phi = awkward.fromiter(df['jet_phi'])\n",
    "    e   = awkward.fromiter(df['jet_E'])\n",
    "    df['jet_n60'] = pt[pt > 60000].counts\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function let you add lepton variables. The jagged arrays need to be turned into variables for each lepton. If you apply skimming on 2 leptons the information of the two hardest leptons will be saved. If skimming is set to 3 it saves the 3 hardest leptons etc. One can also add higher level variables like mll, deltaR etc. See documentation here: [here](https://github.com/scikit-hep/uproot#multiple-values-per-event-jagged-arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lepaugmentation(df,events,prev_n,next_n,nlep):\n",
    "    #pt, eta, phi, e = events.arrays(['lep_pt','lep_eta','lep_phi','lep_E'],outputtype=tuple,entrystart=prev_n,entrystop=next_n)\n",
    "    \n",
    "    #print(df.keys())\n",
    "    #subset = df[['lep_pt']]#,'lep_eta','lep_phi','lep_E']]\n",
    "    #pt = tuple(subset.values)\n",
    "    #pt = fromparents(df['lep_pt'])\n",
    "    pt  = awkward.fromiter(df['lep_pt'])\n",
    "    eta = awkward.fromiter(df['lep_eta'])\n",
    "    phi = awkward.fromiter(df['lep_phi'])\n",
    "    e   = awkward.fromiter(df['lep_E'])\n",
    "    \n",
    "    # Compute mll for all combinations of two-lepton pairs\n",
    "    pairs = pt.argchoose(2)\n",
    "    left  = pairs.i0\n",
    "    right = pairs.i1\n",
    "    masses = np.sqrt(2*pt[left]*pt[right]*(np.cosh(eta[left]-eta[right])-np.cos(phi[left]-phi[right])))\n",
    "    df['mll'] = masses.content\n",
    "    \n",
    "    pt = pt.pad(nlep).fillna(-999)\n",
    "    eta = eta.pad(nlep).fillna(-999)\n",
    "    phi = phi.pad(nlep).fillna(-999)\n",
    "    e = e.pad(nlep).fillna(-999)\n",
    "\n",
    "    # Make the lepton variables\n",
    "    for i in range(1,nlep+1):\n",
    "        df['lep%i_pt'%i]  = pt[pt.argmax(),:].flatten()\n",
    "        df['lep%i_eta'%i] = eta[pt.argmax(),:].flatten()\n",
    "        df['lep%i_phi'%i] = phi[pt.argmax(),:].flatten()\n",
    "        df['lep%i_E'%i]   = e[pt.argmax(),:].flatten()\n",
    "        # Remove the hardest and continue to find the next to hardest etc.\n",
    "        mask = np.logical_and(pt != pt.max(), pt.max != -999)\n",
    "        pt  = pt[mask]\n",
    "        eta = eta[mask]\n",
    "        phi = phi[mask]\n",
    "        e   = e[mask]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OpenDataPandaFramework(indir,nlep,chunksize,printevry,datatype,skimtag,isMC,isData,isSignal,lep_ptcut,branches,lumi):\n",
    "    # Count how many events and files we write/read\n",
    "    n_allfiles   = 0\n",
    "    nfile   = 0\n",
    "    totev   = 0\n",
    "    totev_skim = 0\n",
    "    out_filenum = 1\n",
    "    try:\n",
    "        del [result]\n",
    "    except:\n",
    "        print(\"WARNING\\t Result does not exists. Good :-)\")\n",
    "    root_files = [f for f in listdir(indir) if f.endswith('.root')]\n",
    "\n",
    "    # Checking if file exitst (not needed)\n",
    "    #path = indir+\"/%s_%s.h5\" %(datatype,skimtag)\n",
    "    #if os.path.exists(path):\n",
    "    #    os.remove(path)\n",
    "    #    print(\"WARNING\\t Removing file {:s}\".format(path))\n",
    "\n",
    "    for f in root_files:\n",
    "        #if not \"410012\" in f: continue\n",
    "        n_allfiles += 1\n",
    "        # Getting the file and extracting the information from info dictionary\n",
    "        events = uproot.open(indir+\"/\"+f)[\"mini\"]\n",
    "        nentries = events.numentries\n",
    "        print(\"INFO  \\t Opening file {:d}/{:d}: {:s} with {:d} events\".format(n_allfiles,len(root_files),f,nentries))\n",
    "        path = indir+\"/%s_%s.h5\" %(f,skimtag)\n",
    "        if not isData:\n",
    "            file_id = int(f.split('.')[1])\n",
    "            if not file_id in info.keys():\n",
    "                print(\"ERROR \\t Could not find any info for file id {:d}. Skipping.\".format(file_id))\n",
    "                continue\n",
    "            xsec = float(info[file_id]['xsec'])\n",
    "            nev  = float(info[file_id]['sumw'])\n",
    "\n",
    "\n",
    "            # Find the MC type (defined by dictonaries a few cells above)\n",
    "            mccat = \"\" \n",
    "            if isMC:\n",
    "                for key in fileIDs_bkg.keys():\n",
    "                    if file_id in fileIDs_bkg[key]:       \n",
    "                        mccat = key\n",
    "                        break\n",
    "            elif isSignal:\n",
    "                for key in fileIDs_sig.keys():\n",
    "                    if file_id in fileIDs_sig[key]:       \n",
    "                        mccat = key\n",
    "                        break\n",
    "            if not mccat:\n",
    "                print(\"ERROR \\t Could not find category for DSID {:d}. Skipping\".format(file_id))\n",
    "                continue\n",
    "            print(\"INFO  \\t ID {:d} in category {:s} has xsec = {:.1f} fb and nev = {:.2f} \".format(file_id,mccat,xsec,nev))\n",
    "\n",
    "\n",
    "        n = 1\n",
    "        prev_n = 0\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # Measure time to read \n",
    "            if n == 1: start = time.time()   \n",
    "            else:   \n",
    "                end = time.time()\n",
    "                dur = (end - start)\n",
    "                dur_sec = chunksize/dur\n",
    "                m, s = divmod((nentries-((n-1)*chunksize))/dur_sec, 60)\n",
    "                h, m = divmod(m, 60)\n",
    "                print(\"INFO  \\t Event/sec  = {:.0f}. ETC = {:d}h{:02d}m{:02d}s\".format(dur_sec,int(h),int(m),int(s)))\n",
    "                start = time.time()\n",
    "\n",
    "            # Get the range of events to read\n",
    "            next_n = n*chunksize if n*chunksize < nentries else nentries\n",
    "            print(\"INFO  \\t Reading entries {:d} - {:d} of {:d}. Total so far: {:d}\".format(prev_n,next_n,nentries,totev_skim))\n",
    "            df = events.pandas.df(branches,flatten=False,entrystart=prev_n,entrystop=next_n)\n",
    "\n",
    "            #print(df.keys)\n",
    "\n",
    "            totev += len(df.index)\n",
    "\n",
    "            # If MC: get the scale factor corresponding to the total data luminosity (is set above). \n",
    "            # If data: set scale factor to 1!\n",
    "            if not \"data\" in f:\n",
    "                df['wgt'] = np.vectorize(calc_sf)(xsec,lumi,nev,df.mcWeight,\n",
    "                                                  df.scaleFactor_PILEUP,df.scaleFactor_ELE,\n",
    "                                                  df.scaleFactor_MUON,df.scaleFactor_BTAG,\n",
    "                                                  df.scaleFactor_LepTRIGGER)\n",
    "            else:\n",
    "                df['wgt'] = 1.0\n",
    "\n",
    "            # Define if it is signal or background (important for ML classification)\n",
    "            if isMC or isData:\n",
    "                df['isSignal'] = 0\n",
    "            elif isSignal:\n",
    "                df['isSignal'] = 1\n",
    "            else:\n",
    "                df['isSignal'] = 0\n",
    "\n",
    "            if not isData: df['MCType'] = mccat\n",
    "            else: df['MCType'] = \"Data\"\n",
    "\n",
    "            ###########   \n",
    "            # Skimming\n",
    "            ###########  \n",
    "            df = skimming(df,events,prev_n,next_n,nlep,lep_ptcut)\n",
    "\n",
    "            ##############\n",
    "            # Augumenting\n",
    "            ##############\n",
    "            df = jetaugmentation(df,events,prev_n,next_n)\n",
    "            df = lepaugmentation(df,events,prev_n,next_n,nlep)\n",
    "\n",
    "            ###########   \n",
    "            # Slimming\n",
    "            ###########\n",
    "            df = df.drop(['mcWeight','scaleFactor_PILEUP','scaleFactor_ELE','scaleFactor_MUON','scaleFactor_BTAG','scaleFactor_LepTRIGGER'],axis=1)\n",
    "            df = df.drop(['jet_n', 'jet_pt', 'jet_eta', 'jet_phi', 'jet_E', 'jet_jvt'],axis=1)\n",
    "            df = df.drop(['jet_trueflav', 'jet_truthMatched', 'jet_MV2c10', 'jet_pt_syst'],axis=1)\n",
    "            df = df.drop(['lep_n', 'lep_truthMatched', 'lep_trigMatched', 'lep_pt', 'lep_eta'],axis=1)\n",
    "            df = df.drop(['lep_phi', 'lep_E', 'lep_z0', 'lep_charge', 'lep_type', 'lep_isTightID'],axis=1)\n",
    "            df = df.drop(['lep_ptcone30', 'lep_etcone20', 'lep_trackd0pvunbiased'],axis=1)\n",
    "            df = df.drop(['lep_tracksigd0pvunbiased', 'lep_pt_syst'],axis=1)\n",
    "            # If first time, create result data frame. If not; concatenate\n",
    "            try: \n",
    "                result = pd.concat([result,df])\n",
    "            except:\n",
    "                result = df\n",
    "                print(\"WARNING\\t Starting a new result panda\")\n",
    "\n",
    "\n",
    "            totev_skim += len(df.index)\n",
    "\n",
    "            # Delete the temporary data frame\n",
    "            del [df]\n",
    "\n",
    "            if totev_skim > printevry:\n",
    "                #result = sortAndIndex(result,nlep)\n",
    "                path = indir+\"/%s_%s_num_%i.h5\" %(datatype,skimtag,out_filenum)\n",
    "                result.to_hdf(path,'mini',mode='w',table=True)\n",
    "                print(\"INFO  \\t Read {:d} events in {:d} files, for which {:d} ({:.2f}%) were written to {:s}\"\n",
    "                    .format(totev,nfile,totev_skim,(float(totev_skim)/float(totev))*100.,path))\n",
    "                out_filenum += 1\n",
    "                totev = 0\n",
    "                totev_skim = 0\n",
    "                nfile = 0\n",
    "                del [result]\n",
    "\n",
    "            # If read everything\n",
    "            if n*chunksize > nentries: break\n",
    "\n",
    "            # Update counters before continuing\n",
    "            prev_n = n*chunksize + 1\n",
    "            n += 1\n",
    "            #break\n",
    "\n",
    "        nfile += 1\n",
    "        del [events]  \n",
    "        #break\n",
    "    # Make sure we write the last events\n",
    "    #result = sortAndIndex(result,nlep)\n",
    "    path = indir+\"/%s_%s_num_%i.h5\" %(datatype,skimtag,out_filenum)\n",
    "    result.to_hdf(path,'mini',mode='w',table=True)\n",
    "    print(\"INFO  \\t Read {:d} events in {:d} files, for which {:d} ({:.2f}%) were written to {:s}\"\n",
    "        .format(totev,nfile,totev_skim,(float(totev_skim)/float(totev))*100.,path))\n",
    "    #del [result]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
